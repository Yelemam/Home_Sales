{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_KW73O2e3dw",
    "outputId": "fa5fd2b3-e2de-491b-ee1c-405317ba7ebc"
   },
   "outputs": [],
   "source": [
    "# Import findspark and initialize. \n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2XbWNf1Te5fM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/14 12:33:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOJqxG_RPSwp",
    "outputId": "7857ef9f-5b04-405d-f5aa-e535dfe7870c"
   },
   "outputs": [],
   "source": [
    "# 1. Read in the AWS S3 bucket into a DataFrame.\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file using SparkFiles\n",
    "spark.sparkContext.addFile(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- date_built: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- bathrooms: integer (nullable = true)\n",
      " |-- sqft_living: integer (nullable = true)\n",
      " |-- sqft_lot: integer (nullable = true)\n",
      " |-- floors: integer (nullable = true)\n",
      " |-- waterfront: integer (nullable = true)\n",
      " |-- view: integer (nullable = true)\n",
      "\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
      "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
      "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
      "|b672c137-b88c-48b...|2019-10-16|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
      "|e0726d4d-d595-407...|2022-01-08|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the dataset into a DataFrame\n",
    "file_path = SparkFiles.get(\"home_sales_revised.csv\")\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display the schema and preview the data\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# Save the raw DataFrame as Parquet\n",
    "df.write.parquet(\"output/home_sales_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "RoljcJ7WPpnm"
   },
   "outputs": [],
   "source": [
    "# 2. Create a temporary view of the DataFrame.\n",
    "df.createOrReplaceTempView(\"home_sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6fkwOeOmqvq",
    "outputId": "bdded620-79c4-488d-c7a5-91c6799c419e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|year_sold|avg_price|\n",
      "+---------+---------+\n",
      "|     2019| 300263.7|\n",
      "|     2020|298353.78|\n",
      "|     2021|301819.44|\n",
      "|     2022|296363.88|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. What is the average price for a four bedroom house sold per year, rounded to two decimal places?\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        year(date) AS year_sold, \n",
    "        ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_sales\n",
    "    WHERE bedrooms = 4\n",
    "    GROUP BY year_sold\n",
    "    ORDER BY year_sold\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "The average price for four-bedroom homes fluctuated slightly between 2019 and 2022.\n",
    "The highest average price occurred in 2021 at $301,819.44.\n",
    "The lowest average price occurred in 2022 at $296,363.88, indicating a slight dip in prices.\n",
    "This trend suggests that prices remained relatively stable over the years, with small variations likely due to market conditions or demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8p_tUS8h8it",
    "outputId": "65806e5f-6262-41c0-ff65-5107464e5c4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|year_built|avg_price|\n",
      "+----------+---------+\n",
      "|      2010|292859.62|\n",
      "|      2011|291117.47|\n",
      "|      2012|293683.19|\n",
      "|      2013|295962.27|\n",
      "|      2014|290852.27|\n",
      "|      2015| 288770.3|\n",
      "|      2016|290555.07|\n",
      "|      2017|292676.79|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. What is the average price of a home for each year the home was built,\n",
    "# that have 3 bedrooms and 3 bathrooms, rounded to two decimal places?\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        date_built AS year_built,\n",
    "        ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_sales\n",
    "    WHERE bedrooms = 3 AND bathrooms = 3\n",
    "    GROUP BY date_built\n",
    "    ORDER BY date_built\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Analysis**\n",
    "The average price for homes with 3 bedrooms and 3 bathrooms remained relatively consistent between 2010 and 2017.\n",
    "The highest average price occurred in 2013 at $295,962.27.\n",
    "The lowest average price occurred in 2015 at $288,770.30.\n",
    "The data suggests a slight dip in prices around 2014–2015, followed by stabilization in later years.\n",
    "Overall, the variations in price across the years appear to be minor, indicating a stable market for these homes during this period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-Eytz64liDU",
    "outputId": "17119810-56ad-40c3-de5e-c3db57e43bcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|year_built|avg_price|\n",
      "+----------+---------+\n",
      "|      2010|285010.22|\n",
      "|      2011|276553.81|\n",
      "|      2012|307539.97|\n",
      "|      2013|303676.79|\n",
      "|      2014|298264.72|\n",
      "|      2015|297609.97|\n",
      "|      2016| 293965.1|\n",
      "|      2017|280317.58|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. What is the average price of a home for each year the home was built,\n",
    "# that have 3 bedrooms, 3 bathrooms, with two floors,\n",
    "# and are greater than or equal to 2,000 square feet, rounded to two decimal places?\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        date_built AS year_built,\n",
    "        ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_sales\n",
    "    WHERE bedrooms = 3 \n",
    "      AND bathrooms = 3\n",
    "      AND floors = 2\n",
    "      AND sqft_living >= 2000\n",
    "    GROUP BY date_built\n",
    "    ORDER BY date_built\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "The average price for homes meeting the criteria shows a peak in 2012 at $307,539.97.\n",
    "The lowest average price occurred in 2011, with $276,553.81.\n",
    "Between 2012 and 2015, the average prices remained relatively stable, staying close to or slightly above $300,000.\n",
    "By 2017, the average price decreased to $280,317.58, indicating a slight downward trend in prices for this category of homes.\n",
    "Overall, homes meeting these criteria appear to have experienced small price fluctuations across the years, with a noticeable dip in 2011 and a peak in 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUrfgOX1pCRd",
    "outputId": "17c25774-855e-4290-a4bd-a04902bdc13a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|view_rating| avg_price|\n",
      "+-----------+----------+\n",
      "|        100| 1026669.5|\n",
      "|         99|1061201.42|\n",
      "|         98|1053739.33|\n",
      "|         97|1129040.15|\n",
      "|         96|1017815.92|\n",
      "|         95| 1054325.6|\n",
      "|         94| 1033536.2|\n",
      "|         93|1026006.06|\n",
      "|         92| 970402.55|\n",
      "|         91|1137372.73|\n",
      "|         90|1062654.16|\n",
      "|         89|1107839.15|\n",
      "|         88|1031719.35|\n",
      "|         87| 1072285.2|\n",
      "|         86|1070444.25|\n",
      "|         85|1056336.74|\n",
      "|         84|1117233.13|\n",
      "|         83|1033965.93|\n",
      "|         82| 1063498.0|\n",
      "|         81|1053472.79|\n",
      "+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "--- 0.2615368366241455 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# 6. What is the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000? Order by descending view rating. \n",
    "# Although this is a small dataset, determine the run time for this query.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# SQL query to calculate the average price per view rating\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        view AS view_rating,\n",
    "        ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_sales\n",
    "    GROUP BY view\n",
    "    HAVING AVG(price) >= 350000\n",
    "    ORDER BY view_rating DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show the result\n",
    "result.show()\n",
    "\n",
    "# Stop the timer and print the run time\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "View ratings 91 and 97 had the highest average prices, with $1,137,372.73 and $1,129,040.15, respectively.\n",
    "The lowest average price in this range of view ratings is $970,402.55, which corresponds to view rating 92.\n",
    "The results indicate that homes with high view ratings consistently command premium prices, with most averages exceeding $1,000,000.\n",
    "The run time for the query was efficient at 0.2615 seconds, demonstrating Spark’s capability to handle large datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAhk3ZD2tFy8",
    "outputId": "0a8f132d-40a8-4bd4-b5f2-2847e98427f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
      "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
      "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
      "|b672c137-b88c-48b...|2019-10-16|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
      "|e0726d4d-d595-407...|2022-01-08|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
      "|5aa00529-0533-46b...|2019-01-30|      2017|218712|       2|        3|       1965|   14375|     2|         0|   7|\n",
      "|131492a1-72e2-4a8...|2020-02-08|      2017|419199|       2|        3|       2062|    8876|     2|         0|   6|\n",
      "|8d54a71b-c520-44e...|2019-07-21|      2010|323956|       2|        3|       1506|   11816|     1|         0|  25|\n",
      "|e81aacfe-17fe-46b...|2020-06-16|      2016|181925|       3|        3|       2137|   11709|     2|         0|  22|\n",
      "|2ed8d509-7372-46d...|2021-08-06|      2015|258710|       3|        3|       1918|    9666|     1|         0|  25|\n",
      "|f876d86f-3c9f-42b...|2019-02-27|      2011|167864|       3|        3|       2471|   13924|     2|         0|  15|\n",
      "|0a2bd445-8508-4d8...|2021-12-30|      2014|337527|       2|        3|       1926|   12556|     1|         0|  23|\n",
      "|941bad30-eb49-4a7...|2020-05-09|      2015|229896|       3|        3|       2197|    8641|     1|         0|   3|\n",
      "|dd61eb34-6589-4c0...|2021-07-25|      2016|210247|       3|        2|       1672|   11986|     2|         0|  28|\n",
      "|f1e4cef7-d151-439...|2019-02-01|      2011|398667|       2|        3|       2331|   11356|     1|         0|   7|\n",
      "|ea620c7b-c2f7-4c6...|2021-05-31|      2011|437958|       3|        3|       2356|   11052|     1|         0|  26|\n",
      "|f233cb41-6f33-4b0...|2021-07-18|      2016|437375|       4|        3|       1704|   11721|     2|         0|  34|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|      2015|288650|       2|        3|       2100|   10419|     2|         0|   7|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|      2015|308313|       3|        3|       1960|    9453|     2|         0|   2|\n",
      "|4566cd2a-ac6e-435...|2019-07-15|      2016|177541|       3|        3|       2130|   10517|     2|         0|  25|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Is 'home_sales' cached?  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/14 12:54:35 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# Cache the temporary table home_sales\n",
    "spark.sql(\"CACHE TABLE home_sales\")\n",
    "\n",
    "# Force materialization by running a query\n",
    "spark.sql(\"SELECT * FROM home_sales\").show()\n",
    "\n",
    "# Verify the table is cached\n",
    "print(\"Is 'home_sales' cached? \", spark.catalog.isCached(\"home_sales\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4opVhbvxtL-i",
    "outputId": "38ec8487-795f-4550-b50c-fcc6f2b7c769"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Check if the table is cached.\n",
    "spark.catalog.isCached('home_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GnL46lwTSEk",
    "outputId": "09a16c73-194d-4371-95d1-ee64fe83b91c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|view_rating| avg_price|\n",
      "+-----------+----------+\n",
      "|        100| 1026669.5|\n",
      "|         99|1061201.42|\n",
      "|         98|1053739.33|\n",
      "|         97|1129040.15|\n",
      "|         96|1017815.92|\n",
      "|         95| 1054325.6|\n",
      "|         94| 1033536.2|\n",
      "|         93|1026006.06|\n",
      "|         92| 970402.55|\n",
      "|         91|1137372.73|\n",
      "|         90|1062654.16|\n",
      "|         89|1107839.15|\n",
      "|         88|1031719.35|\n",
      "|         87| 1072285.2|\n",
      "|         86|1070444.25|\n",
      "|         85|1056336.74|\n",
      "|         84|1117233.13|\n",
      "|         83|1033965.93|\n",
      "|         82| 1063498.0|\n",
      "|         81|1053472.79|\n",
      "+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "--- 0.12098884582519531 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# 9. Using the cached data, run the last query above, that calculates \n",
    "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000. \n",
    "# Determine the runtime and compare it to the uncached runtime.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# SQL query to calculate the average price per view rating\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        view AS view_rating,\n",
    "        ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_sales\n",
    "    GROUP BY view\n",
    "    HAVING AVG(price) >= 350000\n",
    "    ORDER BY view_rating DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show the results\n",
    "result.show()\n",
    "\n",
    "# Stop the timer and print the runtime\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "The cached query ran approximately 2.5 times faster than the uncached query.\n",
    "Caching the data significantly reduces query execution time by avoiding repeated reads from the underlying dataset.\n",
    "This demonstrates the effectiveness of caching for iterative or frequent queries in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Qm12WN9isHBR"
   },
   "outputs": [],
   "source": [
    "# 10. Partition by the \"date_built\" field on the formatted parquet home sales data \n",
    "df.write.partitionBy(\"date_built\").parquet(\"home_sales_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home_sales_partitioned\n",
      "home_sales_partitioned/date_built=2013\n",
      "home_sales_partitioned/date_built=2014\n",
      "home_sales_partitioned/date_built=2015\n",
      "home_sales_partitioned/date_built=2012\n",
      "home_sales_partitioned/date_built=2017\n",
      "home_sales_partitioned/date_built=2010\n",
      "home_sales_partitioned/date_built=2011\n",
      "home_sales_partitioned/date_built=2016\n"
     ]
    }
   ],
   "source": [
    "# List the partitioned directory contents\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"home_sales_partitioned\"):\n",
    "    print(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning the data by date_built organizes the dataset into smaller, manageable chunks.\n",
    "This structure improves query performance by allowing Spark to scan only the relevant partitions instead of the entire dataset.\n",
    "Parquet format ensures efficient storage and retrieval for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "AZ7BgY61sRqY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- bathrooms: integer (nullable = true)\n",
      " |-- sqft_living: integer (nullable = true)\n",
      " |-- sqft_lot: integer (nullable = true)\n",
      " |-- floors: integer (nullable = true)\n",
      " |-- waterfront: integer (nullable = true)\n",
      " |-- view: integer (nullable = true)\n",
      " |-- date_built: integer (nullable = true)\n",
      "\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|                  id|      date| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|date_built|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|2ed8d509-7372-46d...|2021-08-06|258710|       3|        3|       1918|    9666|     1|         0|  25|      2015|\n",
      "|941bad30-eb49-4a7...|2020-05-09|229896|       3|        3|       2197|    8641|     1|         0|   3|      2015|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|288650|       2|        3|       2100|   10419|     2|         0|   7|      2015|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|308313|       3|        3|       1960|    9453|     2|         0|   2|      2015|\n",
      "|d715f295-2fbf-4e9...|2021-05-17|391574|       3|        2|       1635|    8040|     2|         0|  10|      2015|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11. Read the formatted parquet data.\n",
    "# Read the Parquet data\n",
    "parquet_df = spark.read.parquet(\"home_sales_partitioned\")\n",
    "\n",
    "# Show the schema to confirm successful loading\n",
    "parquet_df.printSchema()\n",
    "\n",
    "# Preview the first few rows\n",
    "parquet_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Parquet data was successfully loaded into a new DataFrame, parquet_df.\n",
    "The schema matches the original dataset, with date_built recognized as a partition field.\n",
    "This allows for efficient queries, especially when filtering by the partitioned field (date_built).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "J6MJkHfvVcvh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='home_sales', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True), Table(name='home_sales_parquet', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "# 12. Create a temporary table for the parquet data.\n",
    "# Create a temporary table for the Parquet data\n",
    "parquet_df.createOrReplaceTempView(\"home_sales_parquet\")\n",
    "\n",
    "# Verify the table was created by listing all tables\n",
    "print(spark.catalog.listTables())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The temporary table home_sales_parquet was successfully created from the Parquet data.\n",
    "This table allows for SQL-based queries on the Parquet data within the Spark session.\n",
    "Temporary tables are session-specific and need to be recreated in new sessions if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_Vhb52rU1Sn",
    "outputId": "a0b8d0c4-55ed-4c6c-bfd8-4c8c5334838e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|view_rating| avg_price|\n",
      "+-----------+----------+\n",
      "|        100| 1026669.5|\n",
      "|         99|1061201.42|\n",
      "|         98|1053739.33|\n",
      "|         97|1129040.15|\n",
      "|         96|1017815.92|\n",
      "|         95| 1054325.6|\n",
      "|         94| 1033536.2|\n",
      "|         93|1026006.06|\n",
      "|         92| 970402.55|\n",
      "|         91|1137372.73|\n",
      "|         90|1062654.16|\n",
      "|         89|1107839.15|\n",
      "|         88|1031719.35|\n",
      "|         87| 1072285.2|\n",
      "|         86|1070444.25|\n",
      "|         85|1056336.74|\n",
      "|         84|1117233.13|\n",
      "|         83|1033965.93|\n",
      "|         82| 1063498.0|\n",
      "|         81|1053472.79|\n",
      "+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "--- 0.3136897087097168 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# 13. Using the parquet DataFrame, run the last query above, that calculates \n",
    "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000. \n",
    "# Determine the runtime and compare it to the cached runtime.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the query on the Parquet DataFrame\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        view AS view_rating,\n",
    "        ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_sales_parquet\n",
    "    GROUP BY view\n",
    "    HAVING AVG(price) >= 350000\n",
    "    ORDER BY view_rating DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show the results\n",
    "result.show()\n",
    "\n",
    "# Stop the timer and print the runtime\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "Querying the cached table was significantly faster (0.12 seconds) compared to querying the Parquet DataFrame (0.31 seconds).\n",
    "Cached Table:\n",
    "Data is already loaded in memory, reducing I/O operations.\n",
    "Ideal for frequent or iterative queries within the same session.\n",
    "Parquet DataFrame:\n",
    "Data is read from disk, which involves more I/O overhead.\n",
    "Better suited for long-term storage or one-time queries where caching is unnecessary.\n",
    "This comparison highlights the importance of caching for improving query performance when working with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjjYzQGjtbq8",
    "outputId": "830549fd-bb41-451b-9183-5ebf6e3e470b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 14. Uncache the home_sales temporary table.\n",
    "spark.sql(\"UNCACHE TABLE home_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncaching removes the home_sales table from memory but does not delete the table or data.\n",
    "This is useful for freeing up memory when the cached table is no longer needed.\n",
    "Rerunning the notebook will recreate the cache if necessary, ensuring all operations work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sy9NBvO7tlmm",
    "outputId": "be73e0e3-5e85-4794-aad9-025fb6fa84a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 'home_sales' cached?  False\n"
     ]
    }
   ],
   "source": [
    "# 15. Check if the home_sales is no longer cached\n",
    "print(\"Is 'home_sales' cached? \", spark.catalog.isCached(\"home_sales\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The home_sales table was successfully uncached, freeing up memory.\n",
    "Subsequent queries on the table will run without the performance benefit of caching.\n",
    "This step ensures proper memory management, particularly when working with large datasets in Spark.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Home_Sales_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
